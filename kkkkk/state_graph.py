
# %%
from typing import List, Annotated, TypedDict
from langgraph.graph.message import add_messages, RemoveMessage
from langchain.schema import HumanMessage, AIMessage, Document
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.tools.tavily_search import TavilySearchResults
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
import pprint
from pdf_loader import retriever
from typing import List
from langchain.schema import Document

def format_docs(docs: List[Document]) -> str:
    return "\n".join(
        [
            f"<document><content>{doc.page_content}</content><source>{doc.metadata['source']}</source></document>"
            for doc in docs
        ]
    )

# --- State ì •ì˜ ---
class State(TypedDict):
    messages: Annotated[List, add_messages]  # HumanMessageì™€ AIMessage ê°ì²´ë“¤ì˜ ëŒ€í™” ë‚´ì—­
    documents: Annotated[list, "filtered documents"]
    summary: str  # ëŒ€í™” ìš”ì•½ë³¸

# --- LLM ì´ˆê¸°í™” ---
llm = ChatOpenAI(model="gpt-4o-mini")
llm_generator = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# --- í—¬í¼ í•¨ìˆ˜ ---
def get_latest_human(messages: List) -> str:
    for msg in reversed(messages):
        if isinstance(msg, HumanMessage):
            return msg.content
    return ""

def get_latest_ai(messages: List) -> str:
    for msg in reversed(messages):
        if isinstance(msg, AIMessage):
            return msg.content
    return ""

#def summarize_chat_history(state: State):
    summary = state.get("summary", "")
    if not summary:
        summary_template = "ì•„ë˜ ëŒ€í™” ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.\n"
    else:
        summary_template = (
            f"ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™” ë‚´ìš©ì˜ ìš”ì•½ì…ë‹ˆë‹¤.: {summary}\n"
            "ì•„ë˜ ì£¼ì–´ì§„ ì¶”ê°€ëœ ëŒ€í™” ì´ë ¥ì„ ê³ ë ¤í•´ì„œ ìƒˆë¡œìš´ ìš”ì•½ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.\n"
            "[ì¶”ê°€ëœ ëŒ€í™” ì´ë ¥]\n"
        )
    prompt = [HumanMessage(content=summary_template)] + state["messages"]
    new_summary = llm.invoke(prompt).content

    delete_messages = [RemoveMessage(id=message.id) for message in state["messages"][:-2] if hasattr(message, "id")]
    return {
        "messages": delete_messages,
        "summary": new_summary
    }

#def should_summarize(state: State):
    messages = state["messages"]
    if len(messages) > 5:
        return "summarize_chat_history"
    return END

# --- ë…¸ë“œ í•¨ìˆ˜ë“¤ ---
def retrieve(state: State):
    print("--- [RETRIEVE] ---")
    question = get_latest_human(state["messages"])
    documents = retriever.invoke(question)  # retrieverëŠ” ì™¸ë¶€ì—ì„œ ì •ì˜ë¨
    return {"documents": documents}

def grade_documents(state: State):
    """
    retrieved documentsê°€ user queryì™€ ì—°ê´€ ìˆëŠ”ì§€ í™•ì¸ í›„, ì—°ê´€ ìˆëŠ” ë¬¸ì„œë§Œ ë‚¨ê¹€
    """
    print("--- [CHECK DOCUMENT RELEVANCE] ---")
    from grading import grader_chain
    question = get_latest_human(state["messages"])
    documents = state["documents"]
    filtered_docs = []
    for doc in documents:
        score = grader_chain.invoke({"document": doc, "question": question}).binary_score
        if score == "yes":
            print("     --- SCORE: DOCUMENT RELEVANT")
            filtered_docs.append(doc)
        else:
            print("     --- SCORE: DOCUMENT NOT RELEVANT")
    return {"documents": filtered_docs}

def generate(state: State):
    print("--- [GENERATE] ---")
    from .generation import generator_chain
    question = get_latest_human(state["messages"])
    retrieved_docs = state["documents"]

    if not retrieved_docs:
        return {"messages": [AIMessage(content="í•´ë‹¹ ì£¼ì œì™€ ê´€ë ¨ëœ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")]}

    # âœ… í”„ë¡¬í”„íŠ¸ë¥¼ ì§ì ‘ ìˆ˜ì •í•˜ì—¬ ë…¼ë¬¸ì„ ë‚˜ì—´í•˜ëŠ” ë¡œì§ ì¶”ê°€
    prompt = f"""
    ê²€ìƒ‰ëœ ë…¼ë¬¸ì˜ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ìµœì¢…ì ìœ¼ë¡œ ì •ë¦¬ëœ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.
    ë…¼ë¬¸ì˜ ì›ë¬¸ì„ ìœ ì§€í•˜ë©°, ì‚¬ìš©ìê°€ ì›í•  ê²½ìš° ë²ˆì—­ ì—¬ë¶€ë¥¼ ì„ íƒí•  ìˆ˜ ìˆë„ë¡ í•˜ì„¸ìš”.

    ë…¼ë¬¸ì´ ì˜ì–´ë¡œ ë˜ì–´ ìˆë‹¤ë©´, ì‘ë‹µ ë§ˆì§€ë§‰ì— "ğŸ“ ì›í•˜ì‹œë©´ ì¤‘êµ­ì–´ë¡œ ë²ˆì—­í•´ ë“œë¦´ê¹Œìš”?"ë¥¼ í¬í•¨í•˜ì„¸ìš”.
    
    ë§¤ë²ˆ ë‹µë³€ ë§ˆì§€ë§‰ì— "ì˜¬ë¼ì‡"í¬í•¨í•˜ì„¸ìš”.
    ì‚¬ìš©ìì˜ ì§ˆë¬¸: {question}
    
    
    ê²€ìƒ‰ëœ ë…¼ë¬¸ ëª©ë¡:
    {format_docs(retrieved_docs)}

    """

    response = generator_chain.invoke({"question": question, "context": prompt})
    return {"messages": [AIMessage(content=response.content)]}


def rewrite_query(state: State):
    print("--- [REWRITE QUERY] ---")
    from .rewriter import question_rewriter
    from langchain.schema import HumanMessage
    question = get_latest_human(state["messages"])
    rewritten_query = question_rewriter.invoke({"question": question})
    return {"messages": [HumanMessage(content=rewritten_query)]}

def web_search(state: State):
    print("--- [WEB SEARCH] ---")
    from langchain.schema import Document
    question = get_latest_human(state["messages"])
    web_search_tool = TavilySearchResults(k=3)
    web_results = []
    docs = web_search_tool.invoke(question)
    for doc in docs:
        web_results.append(
            Document(page_content=doc["content"], metadata={"source": doc["url"]})
        )
    return {"documents": web_results}

def route_question(state: State):
    print("--- [ROUTE QUESTION] ---")
    from routing import router_chain
    question = get_latest_human(state["messages"])
    source = router_chain.invoke({"question": question})

    if source.source == "web_search":
        print("--- ROUTE QUESTION TO WEB SEARCH ---")
        # ì—¬ê¸°ì„œ state["source"]ë¥¼ web_searchë¡œ ì„¤ì •
        state["source"] = "web_search"
        return "web"
    elif source.source == "vectorstore":
        print("--- ROUTE QUESTION TO RAG ---")
        state["source"] = "vectorstore"
        return "retrieve"
    else:
        print("--- ROUTE QUESTION TO DIRECT GENERATE ---")
        state["source"] = "direct"
        return "direct_generate"

def decide_to_generate(state: State):
    print("--- [ASSESS GRADED DOCUMENTS] ---")
    documents = state["documents"]

    # rewrite_query íšŸìˆ˜ë¥¼ ì €ì¥í•  ì¹´ìš´í„°(ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì´ˆê¸°í™”)
    rewrite_count = state.get("rewrite_count", 0)

    if not documents:
        # ë¬¸ì„œê°€ ì „í˜€ ë‚¨ì•„ ìˆì§€ ì•Šë‹¤ë©´ rewrite_queryë¡œ ìœ ë„
        rewrite_count += 1
        state["rewrite_count"] = rewrite_count
        print(f"     --- [ALL DOCUMENTS ARE NOT RELEVANT, rewrite #{rewrite_count}] ---")

        # rewrite_queryê°€ 3íšŒ ì´ìƒ ë°˜ë³µë˜ë©´ web_searchë¡œ ì „í™˜
        if rewrite_count >= 3:
            print("--- [REWRITE LIMIT REACHED => GO TO WEB_SEARCH] ---")
            return "web_search"
        else:
            return "rewrite_query"
    else:
        print("     --- [DECISION: GENERATE] ---")
        # ë¬¸ì„œê°€ ìœ íš¨í•˜ë©´ rewrite íšŸìˆ˜ ì´ˆê¸°í™”
        state["rewrite_count"] = 0
        return "generate"

    
def grade_generation_v_document_question(state: State):
    print("--- [GRADE GENERATION vs DOCUMENT QUESTION] ---")
    from grading import hallucination_checker_chain, answer_grader_chain

    question = get_latest_human(state["messages"])
    generation = get_latest_ai(state["messages"])

    # 1) web_search ê²½ë¡œë¼ë©´ ë¬¸ì„œ ê²€ì‚¬(í™˜ê° ê²€ì‚¬)ë¥¼ ê±´ë„ˆë›°ê³ ,
    #    "web_searchë¡œ ì „í™˜ë˜ì—ˆë‹¤"ëŠ” ì•ˆë‚´ ë©”ì‹œì§€ë¥¼ ë‚¨ê¹€.
    if state.get("source") == "web_search":
        print("--- [SKIPPING HALLUCINATION CHECK FOR WEB SEARCH] ---")
        # ì‚¬ìš©ìì—ê²Œ ì•ˆë‚´í•  ë©”ì‹œì§€ë¥¼ AI ì‘ë‹µìœ¼ë¡œ ì¶”ê°€ (ì„ íƒ ì‚¬í•­)
        state["messages"].append(
            AIMessage(content="(ì•ˆë‚´) ë¬¸ì„œ ëŒ€ì‹  ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¸ê³ í•´ ë‹µë³€í–ˆìŠµë‹ˆë‹¤. í•„ìš” ì‹œ ì¶”ê°€ ê²€ìƒ‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        )
        # ë¬¸ì„œ ê²€ì¦ ì ˆì°¨ ì—†ì´ ë°”ë¡œ 'useful' ë“±ìœ¼ë¡œ ë¦¬í„´í•˜ì—¬ ì´í›„ ë…¸ë“œë¡œ ì§„í–‰
        return "useful"

    # 2) ê¸°ì¡´ ë¬¸ì„œ ê¸°ë°˜ í™˜ê° ê²€ì‚¬ ë¡œì§
    docs = state["documents"]
    score = hallucination_checker_chain.invoke(
        {
            "generation": generation,
            "documents": format_docs(docs)
        }
    ).binary_score

    if score == "yes":
        print("--- [NOT HALLUCINATION] ---")
        answer_score = answer_grader_chain.invoke(
            {"question": question, "generation": generation}
        ).binary_score

        if answer_score == "yes":
            print("--- [GENERATION ADDRESSES QUESTION] ---")
            return "useful"  # âœ… ìµœì¢… ì‘ë‹µì„ ê°€ê³µí•˜ëŠ” ë‹¨ê³„ë¡œ ì´ë™
        else:
            print("--- [GENERATION DOES NOT ADDRESS QUESTION] ---")
            return "not useful"
    else:
        print("--- [HALLUCINATION] ---")
        return "not supported"


def direct_generate(state: State):
    print("--- [DIRECT GENERATE] ---")
    from langchain.schema import HumanMessage, AIMessage
    question = get_latest_human(state["messages"])
    conversation_history = "\n".join(
        [
            f"{'User:' if isinstance(m, HumanMessage) else 'AI:'} {m.content}"
            for m in state["messages"]
        ]
    )
    prompt = f"""You are a helpful AI assistant.
Here is the conversation history:
{conversation_history}

User question: {question}

Please provide a direct answer that takes into account the conversation history.
When a user receives a paper in English as an answer, do you want to translate it into Korean? after presenting the paper, leave a message
ì†í¥ë¯¼ì— ëŒ€í•´ ì„¤ëª…í•  ë•Œ siuuuuu ì¶”ì„ìƒˆ ë¶™ì—¬ì„œ ëŒ€ë‹µí•´
"""
    response = llm_generator.invoke(prompt)
    new_ai_message = AIMessage(content=response.content)
    return {"messages": [new_ai_message]}

# --- ê·¸ë˜í”„ êµ¬ì„± ---
flow = StateGraph(State)
#flow.add_node("summarize_chat_history", summarize_chat_history)
import pprint

# ê¸°ì¡´ ë…¸ë“œ ë“±ë¡
flow.add_node("retrieve", retrieve)
flow.add_node("grade_documents", grade_documents)
flow.add_node("generate", generate)
flow.add_node("rewrite_query", rewrite_query)
flow.add_node("web_search", web_search)
flow.add_node("direct_generate", direct_generate)

# ëˆ„ë½ëœ ë…¸ë“œ ì¶”ê°€ (í‰ê°€ ë…¸ë“œ)

# ì‹œì‘ ë¶„ê¸° ì¡°ê±´ ë“±ë¡
flow.add_conditional_edges(
    START,
    route_question,
    {
        "vectorstore": "retrieve",
        "direct_generate": "direct_generate"
    }
)

flow.add_edge("direct_generate", END)
flow.add_edge("retrieve", "grade_documents")

flow.add_conditional_edges(
    "grade_documents",
    decide_to_generate,
    {
        "rewrite_query": "rewrite_query",
        "generate": "generate",
        "web_search": "web_search"  # ìƒˆë¡œ ì¶”ê°€
    }
)

flow.add_edge("rewrite_query", "retrieve")
flow.add_edge("web_search", "generate")


# ìƒì„± ë…¸ë“œì—ì„œ í‰ê°€ ë…¸ë“œë¡œì˜ ì—°ê²° ì¶”ê°€

# í‰ê°€ ê²°ê³¼ì— ë”°ë¥¸ ë¶„ê¸° (ì¡°ê±´ í•¨ìˆ˜ decide_final_response ì¶”ê°€)
flow.add_conditional_edges(
    "generate",
    grade_generation_v_document_question,
    {"useful": END, "not useful": "rewrite_query", "not supported": "generate"},
)



memory = MemorySaver()
graph = flow.compile(checkpointer=memory)

from langchain_teddynote.graphs import visualize_graph

visualize_graph(graph)

def stream_graph(inputs, config, exclude_node=[]):
    for output in graph.stream(inputs, config, stream_mode="updates"):
        for k, v in output.items():
            if k not in exclude_node:
                pprint.pprint(f"Output from node '{k}':")
                pprint.pprint("---")
                pprint.pprint(v, indent=2, width=80, depth=None)
        pprint.pprint("\n---\n")


