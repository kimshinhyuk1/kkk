

from typing import List, Annotated, TypedDict
from langgraph.graph.message import add_messages, RemoveMessage
from langchain.schema import HumanMessage, AIMessage, Document
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.tools.tavily_search import TavilySearchResults
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
import pprint
from .pdf_loader import retriever
from typing import List
from langchain.schema import Document

def format_docs(docs: List[Document]) -> str:
    return "\n".join(
        [
            f"<document><content>{doc.page_content}</content><source>{doc.metadata['source']}</source></document>"
            for doc in docs
        ]
    )

# --- State ì •ì˜ ---
class State(TypedDict):
    messages: Annotated[List, add_messages]  # HumanMessageì™€ AIMessage ê°ì²´ë“¤ì˜ ëŒ€í™” ë‚´ì—­
    documents: Annotated[list, "filtered documents"]
    summary: str  # ëŒ€í™” ìš”ì•½ë³¸

# --- LLM ì´ˆê¸°í™” ---
llm = ChatOpenAI(model="gpt-4o-mini")
llm_generator = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# --- í—¬í¼ í•¨ìˆ˜ ---
def get_latest_human(messages: List) -> str:
    for msg in reversed(messages):
        if isinstance(msg, HumanMessage):
            return msg.content
    return ""

def get_latest_ai(messages: List) -> str:
    for msg in reversed(messages):
        if isinstance(msg, AIMessage):
            return msg.content
    return ""

#def summarize_chat_history(state: State):
    summary = state.get("summary", "")
    if not summary:
        summary_template = "ì•„ë˜ ëŒ€í™” ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.\n"
    else:
        summary_template = (
            f"ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™” ë‚´ìš©ì˜ ìš”ì•½ì…ë‹ˆë‹¤.: {summary}\n"
            "ì•„ë˜ ì£¼ì–´ì§„ ì¶”ê°€ëœ ëŒ€í™” ì´ë ¥ì„ ê³ ë ¤í•´ì„œ ìƒˆë¡œìš´ ìš”ì•½ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.\n"
            "[ì¶”ê°€ëœ ëŒ€í™” ì´ë ¥]\n"
        )
    prompt = [HumanMessage(content=summary_template)] + state["messages"]
    new_summary = llm.invoke(prompt).content

    delete_messages = [RemoveMessage(id=message.id) for message in state["messages"][:-2] if hasattr(message, "id")]
    return {
        "messages": delete_messages,
        "summary": new_summary
    }

#def should_summarize(state: State):
    messages = state["messages"]
    if len(messages) > 5:
        return "summarize_chat_history"
    return END

def interaction(state: State):


    # ì²´ì¸(ì˜ˆ: interaction_chain) ì„í¬íŠ¸
    from .interaction import interaction_chain
    from langchain.schema import AIMessage
    question = get_latest_human(state["messages"])
    
    chain_output = interaction_chain.invoke({"question": question})
    final_query_str = chain_output["text"]  # ë˜ëŠ” "final_answer", "response", "content" ë“±
    
    return {
        "messages": [AIMessage(content=final_query_str)],
        # ì¶”ê°€: stateì— ë³„ë„ë¡œ ì €ì¥
        "refined_query": final_query_str
    }


# --- ë…¸ë“œ í•¨ìˆ˜ë“¤ ---
def retrieve(state: State):
    print("--- [RETRIEVE] ---")
    # ë§Œì•½ 'refined_query'ê°€ stateì— ìˆìœ¼ë©´ ê·¸ê±¸ ì‚¬ìš©, ì—†ìœ¼ë©´ user ì§ˆë¬¸ fallback
    refined_query = state.get("refined_query")
    if refined_query:
        question = refined_query
    else:
        question = get_latest_human(state["messages"])
    
    documents = retriever.invoke(question)
    return {"documents": documents}

def grade_documents(state: State):
    """
    Processes retrieved documents without relevance scoring
    """
    print("--- [PROCESS DOCUMENTS] ---")
    
    # Use refined_query if available in state, otherwise use the latest human message
    if state.get("refined_query"):
        question = state["refined_query"]
    else:
        question = get_latest_human(state["messages"])
    
    documents = state["documents"]
    
    # Simply return the documents without filtering
    return {"documents": documents}
def generate(state: State):
    print("--- [GENERATE] ---")
    from .generation import generator_chain
    question = get_latest_human(state["messages"])
    retrieved_docs = state["documents"]

    if not retrieved_docs:
        return {"messages": [AIMessage(content="í•´ë‹¹ ì£¼ì œì™€ ê´€ë ¨ëœ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")]}

    # âœ… í”„ë¡¬í”„íŠ¸ë¥¼ ì§ì ‘ ìˆ˜ì •í•˜ì—¬ ë…¼ë¬¸ì„ ë‚˜ì—´í•˜ëŠ” ë¡œì§ ì¶”ê°€
    prompt = f"""
    ê²€ìƒ‰ëœ ë…¼ë¬¸ì˜ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ìµœì¢…ì ìœ¼ë¡œ ì •ë¦¬ëœ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.
    ë…¼ë¬¸ì˜ ì›ë¬¸ì„ ìœ ì§€í•˜ë©°, ì‚¬ìš©ìê°€ ì›í•  ê²½ìš° ë²ˆì—­ ì—¬ë¶€ë¥¼ ì„ íƒí•  ìˆ˜ ìˆë„ë¡ í•˜ì„¸ìš”.

    ë…¼ë¬¸ì´ ì˜ì–´ë¡œ ë˜ì–´ ìˆë‹¤ë©´, ì‘ë‹µ ë§ˆì§€ë§‰ì— "ğŸ“ ì›í•˜ì‹œë©´ í•œêµ­êµ­ì–´ë¡œ ë²ˆì—­í•´ ë“œë¦´ê¹Œìš”?"ë¥¼ í¬í•¨í•˜ì„¸ìš”.
    ë§¤ë²ˆ ë‹µë³€ì˜ ë§ˆì§€ë§‰ì— ì¶œì²˜ë¥¼ ë‚¨ê¸°ì„¸ìš”.
        ì‚¬ìš©ìì˜ ì§ˆë¬¸: {question}
    
    
    ê²€ìƒ‰ëœ ë…¼ë¬¸ ëª©ë¡:
    {format_docs(retrieved_docs)}

    """

    response = generator_chain.invoke({"question": question, "context": prompt})
    return {"messages": [AIMessage(content=response.content)]}


def rewrite_query(state: State):
    print("--- [REWRITE QUERY] ---")
    from .rewriter import question_rewriter
    from langchain.schema import HumanMessage
    question = get_latest_human(state["messages"])
    rewritten_query = question_rewriter.invoke({"question": question})
    return {"messages": [HumanMessage(content=rewritten_query)]}

def web_search(state: State):
    print("--- [WEB SEARCH] ---")
    
    from langchain.schema import Document
    # ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ ì¶”ì¶œ
    question = get_latest_human(state["messages"])
    
    # ì›¹ ê²€ìƒ‰ì— ì í•©í•œ ì¿¼ë¦¬ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""
    ì‚¬ìš©ìê°€ ìš´ë™ê³¼ ê´€ë ¨í•œ ì§ˆë¬¸ì„ í•˜ì˜€ì„ ë•Œ ê³µì‹ ë ¥ ìˆëŠ” ì‚¬ì´íŠ¸ì—ì„œ ì •ë³´ë¥¼ ê°€ì ¸ì™€ ëŒ€ë‹µí•˜ì‹­ì‹œì˜¤.
    ì‹ ë¢°ì„±,ì „ë¬¸ì„±,ê³µì‹ ë ¥ì´ ë’·ë°›ì¹¨ë˜ëŠ” ìë£Œì´ì—¬ì•¼ í•©ë‹ˆë‹¤.
    ì˜ˆë¥¼ ë“¤ì–´ ì‚¬ìš©ìê°€ ìš´ë™ ê´€ë ¨ ì§ˆë¬¸ì„ í• ê²½ìš° NCAAì™€ ê°™ì€ ê²€ì¦ëœ ì‚¬ì´íŠ¸ì—ì„œ ìë£Œë¥¼ ê°€ì ¸ì™€ ëŒ€ë‹µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
    ì‚¬ìš©ì ì§ˆë¬¸: {question}
    
    ê²€ìƒ‰ ì¿¼ë¦¬:
    """
    # LLMì„ ì‚¬ìš©í•´ ì •ì œëœ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
    refined_query = llm.invoke([HumanMessage(content=prompt)]).content.strip()
    print(f"Refined search query: {refined_query}")
    
    # ìƒì„±ëœ ì¿¼ë¦¬ë¥¼ ì´ìš©í•´ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
    web_search_tool = TavilySearchResults(k=3)
    docs = web_search_tool.invoke(refined_query)
    
    # ê° ê²€ìƒ‰ ê²°ê³¼ê°€ dict ë˜ëŠ” str ì¼ ê²½ìš° ëª¨ë‘ ì²˜ë¦¬
    web_results = []
    for doc in docs:
        if isinstance(doc, dict):
            content = doc.get("content", "")
            source = doc.get("url", "")
        else:
            content = doc
            source = "unknown"
        web_results.append(Document(page_content=content, metadata={"source": source}))
    
    return {"documents": web_results}

def route_question(state: State):
    print("--- [ROUTE QUESTION] ---")
    from routing import router_chain
    question = get_latest_human(state["messages"])
    source = router_chain.invoke({"question": question})

    if source.source == "web_search":
        print("--- ROUTE QUESTION TO WEB SEARCH ---")
        # ì—¬ê¸°ì„œ state["source"]ë¥¼ web_searchë¡œ ì„¤ì •
        state["source"] = "web_search"
        return "web"
    elif source.source == "vectorstore":
        print("--- ROUTE QUESTION TO RAG ---")
        state["source"] = "vectorstore"
        return "retrieve"
    else:
        print("--- ROUTE QUESTION TO DIRECT GENERATE ---")
        state["source"] = "direct"
        return "direct_generate"

def decide_to_generate(state: State):
    print("--- [ASSESS GRADED DOCUMENTS] ---")
    documents = state["documents"]

    # rewrite_query íšŸìˆ˜ë¥¼ ì €ì¥í•  ì¹´ìš´í„°(ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì´ˆê¸°í™”)
    rewrite_count = state.get("rewrite_count", 0)

    if not documents:
        # ë¬¸ì„œê°€ ì „í˜€ ë‚¨ì•„ ìˆì§€ ì•Šë‹¤ë©´ rewrite_queryë¡œ ìœ ë„
        rewrite_count += 1
        state["rewrite_count"] = rewrite_count
        print(f"     --- [ALL DOCUMENTS ARE NOT RELEVANT, rewrite #{rewrite_count}] ---")

        # rewrite_queryê°€ 3íšŒ ì´ìƒ ë°˜ë³µë˜ë©´ web_searchë¡œ ì „í™˜
        if rewrite_count >= 3:
            print("--- [REWRITE LIMIT REACHED => GO TO WEB_SEARCH] ---")
            return "web_search"
        else:
            return "rewrite_query"
    else:
        print("     --- [DECISION: GENERATE] ---")
        # ë¬¸ì„œê°€ ìœ íš¨í•˜ë©´ rewrite íšŸìˆ˜ ì´ˆê¸°í™”
        state["rewrite_count"] = 0
        return "generate"
    
def grade_generation_v_document_question(state: State):
    print("--- [GRADE GENERATION vs DOCUMENT QUESTION] ---")
    from grading import hallucination_checker_chain, answer_grader_chain

    question = get_latest_human(state["messages"])
    generation = get_latest_ai(state["messages"])



    # 1) web_search ê²½ë¡œë¼ë©´ ë¬¸ì„œ ê²€ì‚¬(í™˜ê° ê²€ì‚¬)ë¥¼ ê±´ë„ˆë›°ê³ ,
    #    "web_searchë¡œ ì „í™˜ë˜ì—ˆë‹¤"ëŠ” ì•ˆë‚´ ë©”ì‹œì§€ë¥¼ ë‚¨ê¹€.
    if state.get("source") == "web_search":
        print("--- [SKIPPING HALLUCINATION CHECK FOR WEB SEARCH] ---")
        # ì‚¬ìš©ìì—ê²Œ ì•ˆë‚´í•  ë©”ì‹œì§€ë¥¼ AI ì‘ë‹µìœ¼ë¡œ ì¶”ê°€ (ì„ íƒ ì‚¬í•­)
        state["messages"].append(
            AIMessage(content="(ì•ˆë‚´) ë¬¸ì„œ ëŒ€ì‹  ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¸ê³ í•´ ë‹µë³€í–ˆìŠµë‹ˆë‹¤. í•„ìš” ì‹œ ì¶”ê°€ ê²€ìƒ‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        )
        # ë¬¸ì„œ ê²€ì¦ ì ˆì°¨ ì—†ì´ ë°”ë¡œ 'useful' ë“±ìœ¼ë¡œ ë¦¬í„´í•˜ì—¬ ì´í›„ ë…¸ë“œë¡œ ì§„í–‰
        return "useful"

    # 2) ê¸°ì¡´ ë¬¸ì„œ ê¸°ë°˜ í™˜ê° ê²€ì‚¬ ë¡œì§
    docs = state["documents"]
    score = hallucination_checker_chain.invoke(
        {
            "generation": generation,
            "documents": format_docs(docs)
        }
    ).binary_score

    if score == "yes":
        print("--- [NOT HALLUCINATION] ---")
        answer_score = answer_grader_chain.invoke(
            {"question": question, "generation": generation}
        ).binary_score

        if answer_score == "yes":
            print("--- [GENERATION ADDRESSES QUESTION] ---")
            return "useful"  # âœ… ìµœì¢… ì‘ë‹µì„ ê°€ê³µí•˜ëŠ” ë‹¨ê³„ë¡œ ì´ë™
        else:
            print("--- [GENERATION DOES NOT ADDRESS QUESTION] ---")
            return "not useful"
    else:
        print("--- [HALLUCINATION] ---")
        return "not supported"


def direct_generate(state: State):
    print("--- [DIRECT GENERATE] ---")
    from langchain.schema import HumanMessage, AIMessage
    question = get_latest_human(state["messages"])
    conversation_history = "\n".join(
        [
            f"{'User:' if isinstance(m, HumanMessage) else 'AI:'} {m.content}"
            for m in state["messages"]
        ]
    )
    prompt = f"""You are a helpful AI assistant.
Here is the conversation history:
{conversation_history}

User question: {question}

Please provide a direct answer that takes into account the conversation history.
When a user receives a paper in English as an answer, do you want to translate it into Korean? after presenting the paper, leave a message
"""
    response = llm_generator.invoke(prompt)
    new_ai_message = AIMessage(content=response.content)
    return {"messages": [new_ai_message]}
# --- ê·¸ë˜í”„ êµ¬ì„± ---
flow = StateGraph(State)
#flow.add_node("summarize_chat_history", summarize_chat_history)
import pprint

# ê¸°ì¡´ ë…¸ë“œ ë“±ë¡
flow.add_node("retrieve", retrieve)
flow.add_node("generate", generate)
flow.add_node("rewrite_query", rewrite_query)
flow.add_node("web_search", web_search)
flow.add_node("direct_generate", direct_generate)
flow.add_node("interaction",interaction)
flow.add_node("grade_documents",grade_documents)

flow.add_edge(START, "interaction")  # ì˜ˆì‹œ

flow.add_edge("interaction","retrieve")
flow.add_edge("retrieve", "grade_documents")
flow.add_edge("grade_documents", "generate")
flow.add_edge("generate", END)



memory = MemorySaver()
graph = flow.compile(checkpointer=memory)


def stream_graph(inputs, config, exclude_node=[]):
    for output in graph.stream(inputs, config, stream_mode="updates"):
        for k, v in output.items():
            if k not in exclude_node:
                pprint.pprint(f"Output from node '{k}':")
                pprint.pprint("---")
                pprint.pprint(v, indent=2, width=80, depth=None)
        pprint.pprint("\n---\n")


